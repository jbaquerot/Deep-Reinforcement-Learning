{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.nnet import conv2d\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from scipy.misc import imresize\n",
    "\n",
    "if '../carpole' not in sys.path:\n",
    "    sys.path.append('../carpole')\n",
    "from q_learning_bins import plot_running_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "IM_WIDTH = 80\n",
    "IM_HEIGHT = 80\n",
    "MIN_EXPERIENCES = 5000\n",
    "MAX_EXPERIENCES = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_filter(shape, poolsz):\n",
    "    w = np.random.randn(*shape) / np.sqrt(np.prod(shape[:-1]))\n",
    "    return w.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvPoolLayer(object):\n",
    "    def __init__(self, mi, mo, filtsz= 5, stride= 2, f= T.nnet.relu):\n",
    "        # mi = input feature map size\n",
    "        # mo = output feature map size\n",
    "        sz = (mo, mi, filtsz, filtsz)\n",
    "        W0 = init_filter(sz, stride)\n",
    "        self.W = theano.shared(W0)\n",
    "        b0 = np.zeros(mo, dtype= np.float32)\n",
    "        self.b = theano.shared(b0)\n",
    "        self.stride = (stride, stride)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        conv_out= conv2d(\n",
    "            input = X,\n",
    "            filters = self.W,\n",
    "            subsample= self.stride,\n",
    "        )\n",
    "        return self.f(conv_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image(A):\n",
    "    B = A[31:195] # Select the important part of the image\n",
    "    B = B / 255.0 # scale to 0..1\n",
    "    B = B.mean(axis= 2) # convert to greyscale\n",
    "        \n",
    "        # downsample image\n",
    "        # changing aspect ratio doesn't significantly distort the image\n",
    "        # nearest neighbor interpolation produce a much sharper image\n",
    "        # than default bilinear\n",
    "    B = imresize(B, size= (IM_HEIGHT, IM_WIDTH), interp='nearest')\n",
    "    return B.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a version of HiddenLayer that keeps tracxk of params\n",
    "class HiddenLayer:\n",
    "    def __init__(self, M1, M2, f= T.tanh, use_bias= True):\n",
    "        W = np.random.random((M1, M2)) / np.sqrt(M1*M2)\n",
    "        self.W = theano.shared(W.astype(np.float32))\n",
    "        self.params = [self.W]\n",
    "        self.use_bias = use_bias\n",
    "        if use_bias:\n",
    "            self.b = theano.shared(np.zeros(M2).astype(np.float32))\n",
    "            self.params += [self.b]\n",
    "        self.f = f\n",
    "        \n",
    "    def forward(self, X):\n",
    "        if self.use_bias:\n",
    "            a = X.dot(self.W) + self.b\n",
    "        else:\n",
    "            a = X.dot(self.W)\n",
    "        return self.f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, K, conv_pool_layer_sizes, hidden_layer_sizes, gamma, max_experience, min_experience, batch_sz):\n",
    "        self.K = K\n",
    "        lr = np.float32(2.5e-4)\n",
    "        mu = np.float32(0)\n",
    "        decay = np.float32(0.99)\n",
    "        \n",
    "        # inputs and targets\n",
    "        X = T.ftensor4('X')\n",
    "        G = T.fvector('G')\n",
    "        actions = T.ivector('actions')\n",
    "        \n",
    "        # create the graph\n",
    "        self.conv_leyers = []\n",
    "        num_input_filters = 4 # number of filters / color channels\n",
    "        for num_output_filters, filtersz, stride in conv_pool_layer_sizes:\n",
    "            layer = ConvPoolLayer(num_input_filters, num_output_filters, filtersz, stride)\n",
    "            self.conv_leyers.append(layer)\n",
    "            num_input_filters = num_output_filters\n",
    "            \n",
    "        # get conv output size\n",
    "        Z = X\n",
    "        for layer in self.conv_leyers:\n",
    "            Z = layer.forward(Z)\n",
    "        conv_out = Z.flatten(ndim= 2)\n",
    "        conv_out_op = theano.function(inputs= [X], outputs= conv_out, allow_input_downcast= True)\n",
    "        test = conv_out_op(np.random.randn(1, 4, IM_HEIGHT, IM_WIDTH))\n",
    "        flattened_output_size = test.shape[1]\n",
    "        \n",
    "        #build fully connected layers\n",
    "        self.layers = []\n",
    "        M1 = flattened_output_size\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.layers.append(layer)\n",
    "            M1 = M2\n",
    "            \n",
    "        # final layer\n",
    "        layer = HiddenLayer(M1, K, lambda x: x)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        # collect params for copy\n",
    "        self.params = []\n",
    "        for layer in (self.conv_leyers + self.layers):\n",
    "            self.params += layer.params\n",
    "        caches = [theano.shared(np.ones_like(p.get_value())*0.1) for p in self.params]\n",
    "        velocities = [theano.shared(p.get_value()*0) for p in self.params]\n",
    "        \n",
    "        # calculate final output and cost\n",
    "        Z = conv_out\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        Y_hat = Z\n",
    "        \n",
    "        selected_action_values = Y_hat[T.arange(actions.shape[0]), actions]\n",
    "        cost = T.sum((G - selected_action_values)**2)\n",
    "        \n",
    "        # create train fuction\n",
    "        grads = T.grad(cost, self.params)\n",
    "        g_update = [(p, p + v) for p, v, g in zip(self.params, velocities, grads)]\n",
    "        c_update = [(c, decay*c + (np.float32(1) - decay)*g*g) for c, g in zip(caches, grads)]\n",
    "        v_update = [(v, mu*v - lr*g / T.sqrt(c)) for v, c, g in zip(velocities, caches, grads)]\n",
    "        # v_update = [(v, mu*v - lr*g) for v, g in zip(velocities, grads)]\n",
    "        # c_update = []\n",
    "        \n",
    "        updates = c_update + g_update + v_update\n",
    "        \n",
    "        # compile fuctions\n",
    "        self.train_op = theano.function(\n",
    "            inputs = [X, G, actions],\n",
    "            updates = updates,\n",
    "            allow_input_downcast= True\n",
    "        )\n",
    "        \n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [X],\n",
    "            outputs = Y_hat,\n",
    "            allow_input_downcast = True\n",
    "        )\n",
    "\n",
    "        # create replay memory\n",
    "        self.experience = []\n",
    "        self.max_experience = max_experience\n",
    "        self.min_experience = min_experience\n",
    "        self.batch_sz = batch_sz\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def copy_from(self, other):\n",
    "        for p, q in zip(self.params, other.params):\n",
    "            actual = q.get_value()\n",
    "            p.set_value(actual)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return self.predict_op(X)\n",
    "    \n",
    "    def train(self, target_network):\n",
    "        # sample a random batch from buffer, do an iteration of GD\n",
    "        if len(self.experience) < self.min_experience:\n",
    "            # don't do anything if we don't have enough experience\n",
    "            return\n",
    "        \n",
    "        # randomly select a batch\n",
    "        sample = random.sample(self.experience, self.batch_sz)\n",
    "        states, actions, rewards, next_states = map(np.array, zip(*sample))\n",
    "        next_Q = np.max(target_network.predict(next_states), axis= 1)\n",
    "        targets = [r + self.gamma*next_q for r, next_q in zip(rewards, next_Q)]\n",
    "        \n",
    "        # call optimizer\n",
    "        self.train_op(list(states), list(targets), list(actions))\n",
    "        \n",
    "    def add_experience(self, s, a, r, s2):\n",
    "        if len(self.experience) >= self.max_experience:\n",
    "            self.experience.pop(0)\n",
    "        self.experience.append((s, a, r, s2))\n",
    "        \n",
    "    def sample_action(self, x, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            return np.argmax(self.predict([x])[0])\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(state, observation):\n",
    "    # downsample and grayscale observation\n",
    "    observation_small = downsample_image(observation)\n",
    "    state.append(observation_small)\n",
    "    if len(state) > 4:\n",
    "        state.pop(0)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(env, model, tmodel, eps, eps_step, gamma, copy_period):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    state = []\n",
    "    prev_state = []\n",
    "    update_state(state, observation) # add the first observation\n",
    "    \n",
    "    while not done and iters < 2000:\n",
    "        # if we reach 2000, just quit, don't want this going forever\n",
    "        # the 200 limit seems a bit early\n",
    "        \n",
    "        if len(state) < 4:\n",
    "            # we can't choose an action base on model\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = model.sample_action(state, eps)\n",
    "            \n",
    "        # copy state to prev state\n",
    "        prev_state.append(state[-1])\n",
    "        if len(prev_state) > 4:\n",
    "            prev_state.pop(0)\n",
    "            \n",
    "        # perform the action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # add the frame to the state\n",
    "        update_state(state, observation)\n",
    "        \n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            reward = -200\n",
    "        \n",
    "        # update the model\n",
    "        model.add_experience(prev_state, action, reward, state)\n",
    "        model.train(tmodel)\n",
    "        \n",
    "        iters += 1\n",
    "        eps = max(eps - eps_step, 0.1)\n",
    "        \n",
    "        if iters % copy_period == 0:\n",
    "            tmodel.copy_from(model)\n",
    "            \n",
    "    return totalreward, eps, iters\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "gamma = 0.99\n",
    "copy_period = 400\n",
    "    \n",
    "D = len(env.observation_space.sample())\n",
    "K = env.action_space.n\n",
    "conv_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "hidden_sizes = [512]\n",
    "batch_sz = 32\n",
    "model = DQN(K, conv_sizes, hidden_sizes, gamma, MAX_EXPERIENCES, MIN_EXPERIENCES, batch_sz)\n",
    "target_model = DQN(K, conv_sizes, hidden_sizes, gamma, MAX_EXPERIENCES, MIN_EXPERIENCES, batch_sz)\n",
    "    \n",
    "if 'monitor' in sys.argv:\n",
    "    filename = os.path.basename(__file__).split('.')[0]\n",
    "    monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "    env = wrappers.Monitor(env, monitor_dir)\n",
    "        \n",
    "num_episodes = 1000 # 100000\n",
    "totalreward = np.empty(num_episodes)\n",
    "costs = np.empty(num_episodes)\n",
    "    \n",
    "eps = 1.0\n",
    "n_max = 500000 # last step to decreasy epsion\n",
    "eps_step = 0.9 / n_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating experience replay buffer...\n"
     ]
    }
   ],
   "source": [
    "### Populating experience replay buffer\n",
    "print(\"Populating experience replay buffer...\")\n",
    "obs = env.reset()\n",
    "obs_small = downsample_image(obs)\n",
    "#state = []\n",
    "state = np.stack([obs_small] * 4, axis=0).tolist()\n",
    "# assert(state.shape == (4, 80, 80))\n",
    "    \n",
    "for i in range(MIN_EXPERIENCES):\n",
    "\n",
    "    action = np.random.choice(K)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    next_state = update_state(state, obs)\n",
    "    # assert(state.shape == (4, 80, 80))\n",
    "    model.add_experience(state, action, reward, next_state)\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        obs_small = downsample_image(obs)\n",
    "        state = np.stack([obs_small] * 4, axis=0).tolist()\n",
    "        # assert(state.shape == (4, 80, 80))\n",
    "    else:\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_t, episode_reward, num_steps_in_episode= play_one(\n",
    "            env,\n",
    "            model,\n",
    "            target_model,\n",
    "            eps,\n",
    "            eps_step,\n",
    "            gamma,\n",
    "            copy_period\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p치rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0d1a34a8ab8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp치rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'p치rate' is not defined"
     ]
    }
   ],
   "source": [
    "p치rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Breakout-v0')\n",
    "    gamma = 0.99\n",
    "    copy_period = 400\n",
    "    \n",
    "    D = len(env.observation_space.sample())\n",
    "    K = env.action_space.n\n",
    "    conv_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "    hidden_sizes = [512]\n",
    "    batch_sz = 32\n",
    "    model = DQN(K, conv_sizes, hidden_sizes, gamma, MAX_EXPERIENCES, MIN_EXPERIENCES, batch_sz)\n",
    "    target_model = DQN(K, conv_sizes, hidden_sizes, gamma, MAX_EXPERIENCES, MIN_EXPERIENCES, batch_sz)\n",
    "    \n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "        \n",
    "    num_episodes = 1000 # 100000\n",
    "    totalreward = np.empty(num_episodes)\n",
    "    costs = np.empty(num_episodes)\n",
    "    \n",
    "    eps = 1.0\n",
    "    n_max = 500000 # last step to decreasy epsion\n",
    "    eps_step = 0.9 / n_max\n",
    "    \n",
    "    ### Populating experience replay buffer\n",
    "    print(\"Populating experience replay buffer...\")\n",
    "    obs = env.reset()\n",
    "    obs_small = downsample_image(obs)\n",
    "    #state = []\n",
    "    state = np.stack([obs_small] * 4, axis=0).tolist()\n",
    "    # assert(state.shape == (4, 80, 80))\n",
    "    \n",
    "    for i in range(MIN_EXPERIENCES):\n",
    "\n",
    "        action = np.random.choice(K)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        next_state = update_state(state, obs)\n",
    "        # assert(state.shape == (4, 80, 80))\n",
    "        model.add_experience(state, action, reward, next_state)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            obs_small = downsample_image(obs)\n",
    "            state = np.stack([obs_small] * 4, axis=0).tolist()\n",
    "            # assert(state.shape == (4, 80, 80))\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    ### Play a number of episodes and learn!\n",
    "    print(\"Play a number of episodes and learn!\")\n",
    "    for i in range(num_episodes):\n",
    "        total_t, episode_reward, num_steps_in_episode= play_one(\n",
    "            env,\n",
    "            model,\n",
    "            target_model,\n",
    "            eps,\n",
    "            eps_step,\n",
    "            gamma,\n",
    "            copy_period\n",
    "        )\n",
    "        totalreward[i] = episode_reward\n",
    "\n",
    "        last_100_avg = totalreward[max(0, i - 100):i + 1].mean()\n",
    "        print(\"Episode:\", i,\n",
    "            #\"Duration:\", duration,\n",
    "            \"Num steps:\", num_steps_in_episode,\n",
    "            \"Reward:\", episode_reward,\n",
    "            #\"Training time per step:\", \"%.3f\" % time_per_step,\n",
    "            \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg,\n",
    "            #\"Epsilon:\", \"%.3f\" % epsilon\n",
    "        )\n",
    "        sys.stdout.flush()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
