{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from scipy.misc import imresize\n",
    "\n",
    "if '../carpole' not in sys.path:\n",
    "    sys.path.append('../carpole')\n",
    "from q_learning_bins import plot_running_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "IM_WIDTH = 80\n",
    "IM_HEIGHT = 80\n",
    "#MAX_EXPERIENCES = 500000\n",
    "MIN_EXPERIENCES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image(A):\n",
    "    B = A[31:195] # select the import parts of the image\n",
    "    B = B.mean(axis=2) # convert to grayscale\n",
    "    B = B / 255.0 # scale to 0..1\n",
    "    \n",
    "    # downsample image\n",
    "    # changing aspect ratio doen't significantly distort the image\n",
    "    # nearest neighbor interpolation produce a much sharper image\n",
    "    # than default bilinear\n",
    "    B = imresize(B, size= (IM_HEIGHT, IM_WIDTH), interp= 'nearest')\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, K, conv_layer_sizes, hidden_layer_sizes, gamma, scope, max_experiences= 10000, min_experiences= 100, batch_sz= 32):\n",
    "         # K = number of actions\n",
    "        self.K = K\n",
    "        self.scope = scope\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            # input and targets\n",
    "            self.X = tf.placeholder(tf.float32, shape= (None, 4, IM_HEIGHT, IM_WIDTH), name= 'X')\n",
    "            # tensorflow convolution needs the order to be:\n",
    "            # (num_samples, height, width, \"color\")\n",
    "            # so we need to tranpose later\n",
    "            self.G = tf.placeholder(tf.float32, shape= (None,), name= 'G')\n",
    "            self.actions = tf.placeholder(tf.int32, shape= (None, ), name= 'actions')\n",
    "            \n",
    "            # calculate output and cost\n",
    "            # convolutional layers\n",
    "            # these built-in layers are faster and don't require us to\n",
    "            # calculate the size of the output of the final conv layer!\n",
    "            Z = self.X\n",
    "            Z = tf.transpose(Z, [0, 2, 3, 1])\n",
    "            for num_output_filters, filtersz, poolsz in conv_layer_sizes:\n",
    "                Z = tf.contrib.layers.conv2d(\n",
    "                    Z,\n",
    "                    num_output_filters,\n",
    "                    filtersz,\n",
    "                    poolsz,\n",
    "                    activation_fn= tf.nn.relu\n",
    "                )\n",
    "            \n",
    "            # fully connected layers\n",
    "            Z = tf.contrib.layers.flatten(Z)\n",
    "            for M in hidden_layer_sizes:\n",
    "                Z = tf.contrib.layers.fully_connected(Z,M)\n",
    "                \n",
    "            # final output layer\n",
    "            self.predict_op = tf.contrib.layers.fully_connected(Z,K)\n",
    "            \n",
    "            selected_action_values = tf.reduce_sum(\n",
    "                self.predict_op * tf.one_hot(self.actions, K),\n",
    "                reduction_indices = [1]\n",
    "            )\n",
    "            \n",
    "            cost = tf.reduce_sum(tf.square(self.G - selected_action_values))\n",
    "            # self.train_op = tf.train.AdamOptimizer(10e-3).minimize(cost)\n",
    "            # self.train_op = tf.train.AdagradOptimizer(10e-3).minimize(cost)\n",
    "            self.train_op = tf.train.RMSPropOptimizer(2.5e-4, decay= 0.99, epsilon= 10e-3).minimize(cost)\n",
    "            # self.train_op = tf.train.MomentumOptimizer(10e-4, momentum= 0.9).minimize(cost)\n",
    "            # self.train_op = tf.train.GradientDescentOptimizer(10e-5).minimize(cost)\n",
    "            \n",
    "            # create replay memory\n",
    "            self.experience = []\n",
    "            self.max_experiences = max_experiences\n",
    "            self.min_experiences = min_experiences\n",
    "            self.batch_sz = batch_sz\n",
    "            self.gamma = gamma\n",
    "            \n",
    "    def copy_from(self, other):\n",
    "        print(\"coping to the target_network\")\n",
    "        mine = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
    "        mine = sorted(mine, key= lambda v: v.name)\n",
    "        theirs = [t for t in tf.trainable_variables() if t.name.startswith(other.scope)]\n",
    "        theirs = sorted(theirs, key= lambda v: v.name)\n",
    "        \n",
    "        ops = []\n",
    "        for p, q in zip(mine, theirs):\n",
    "            actual = self.session.run(q)\n",
    "            op = p.assign(actual)\n",
    "            ops.append(op)\n",
    "        \n",
    "        self.session.run(ops)\n",
    "        \n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "        \n",
    "    def predict(self, states):\n",
    "        return self.session.run(self.predict_op, feed_dict= {self.X: list(states)})\n",
    "    \n",
    "    def train(self, target_network):\n",
    "        # sample a random batch from buffer, do an iteration of GD\n",
    "        if len(self.experience) < self.min_experiences:\n",
    "            # don't do anything if we don't have enough experience\n",
    "            return\n",
    "        \n",
    "        # randomly select a batch\n",
    "        sample = random.sample(self.experience, self.batch_sz)\n",
    "        states, actions, rewards, next_states = map(np.array, zip(*sample))\n",
    "        next_Q = np.max(target_network.predict(next_states), axis= 1)\n",
    "        targets = [r + self.gamma*next_q for r, next_q in zip(rewards, next_Q)]\n",
    "        \n",
    "        # call optimizer\n",
    "        self.session.run(\n",
    "            self.train_op,\n",
    "            feed_dict ={\n",
    "                self.X: states,\n",
    "                self.G: targets,\n",
    "                self.actions: list(actions)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def add_experience(self, s, a, r, s2):\n",
    "        if len(self.experience) >= self.max_experiences:\n",
    "            self.experience.pop(0)\n",
    "        self.experience.append((s, a, r, s2))\n",
    "        \n",
    "    def sample_action(self, x, eps):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(self.K)\n",
    "        else:\n",
    "            return np.argmax(self.predict([x])[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(state, observation):\n",
    "    # downsample and grayscale observation\n",
    "    observation_small= downsample_image(observation)\n",
    "    state.append(observation_small)\n",
    "    if len(state) > 4:\n",
    "        state.pop(0)\n",
    "    return state\n",
    "    #return np.append(state[1:], np.expand_dims(observation_small, 0), axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one(env, model, tmodel, eps, eps_step, gamma, copy_period):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    state = []\n",
    "    prev_state = []\n",
    "    update_state(state, observation) # add the first observation\n",
    "    while not done: \n",
    "    \n",
    "    #and iters < 2000:\n",
    "        # if we reach 2000, just quit, don't want this going forever\n",
    "        # the 200 limit seems a bit early\n",
    "        \n",
    "        if len(state) < 4:\n",
    "            # we can't choose an action base on model\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = model.sample_action(state, eps)\n",
    "            \n",
    "        # copy state to prev state\n",
    "        prev_state.append(state[-1])\n",
    "        if len(prev_state) > 4:\n",
    "            prev_state.pop(0)\n",
    "            \n",
    "        # perform the action\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # add the new frame to the state\n",
    "        update_state(state, observation)\n",
    "        \n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            reward = -200\n",
    "            \n",
    "        # update the model\n",
    "        model.add_experience(prev_state, action, reward, state)\n",
    "        model.train(tmodel)\n",
    "        \n",
    "        iters += 1\n",
    "        eps = max(eps - eps_step, 0.1) # decrease linealy until 0.1\n",
    "        \n",
    "        if iters % copy_period == 0:\n",
    "            tmodel.copy_from(model)\n",
    "            \n",
    "    return totalreward, eps, iters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('Breakout-v0')\n",
    "    gamma = 0.99\n",
    "    copy_period = 400\n",
    "    \n",
    "    D = len(env.observation_space.sample())\n",
    "    K = env.action_space.n\n",
    "    conv_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]\n",
    "    hidden_sizes = [512]\n",
    "    model = DQN(K, conv_sizes, hidden_sizes, gamma, scope= 'main')\n",
    "    target_model = DQN(K, conv_sizes, hidden_sizes, gamma, scope= 'target')\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.InteractiveSession()\n",
    "    session.run(init)\n",
    "    model.set_session(session)\n",
    "    target_model.set_session(session)\n",
    "    \n",
    "    batch_sz = 32\n",
    "    num_episodes = 10000\n",
    "    total_t = 0\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    # epsilon\n",
    "    # decays linearly until 0.1\n",
    "    eps = 1.0\n",
    "    eps_step = 0.1\n",
    "    \n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "        \n",
    "    print(\"Populating experience replay buffer...\")\n",
    "    obs = env.reset()\n",
    "    obs_small = downsample_image(obs)\n",
    "    #state = []\n",
    "    state = np.stack([obs_small] * 4, axis=0).tolist()\n",
    "    # assert(state.shape == (4, 80, 80))\n",
    "    \n",
    "    for i in range(MIN_EXPERIENCES):\n",
    "\n",
    "        action = np.random.choice(K)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        next_state = update_state(state, obs)\n",
    "        # assert(state.shape == (4, 80, 80))\n",
    "        model.add_experience(state, action, reward, next_state)\n",
    "\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            obs_small = downsample_image(obs)\n",
    "            state = np.stack([obs_small] * 4, axis=0).tolist()\n",
    "            # assert(state.shape == (4, 80, 80))\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "    \n",
    "    # Play a number of episodes and learn!\n",
    "    for i in range(num_episodes):\n",
    "        total_t, episode_reward, num_steps_in_episode= play_one(\n",
    "            env,\n",
    "            model,\n",
    "            target_model,\n",
    "            eps,\n",
    "            eps_step,\n",
    "            gamma,\n",
    "            copy_period\n",
    "        )\n",
    "        episode_rewards[i] = episode_reward\n",
    "\n",
    "        last_100_avg = episode_rewards[max(0, i - 100):i + 1].mean()\n",
    "        print(\"Episode:\", i,\n",
    "            #\"Duration:\", duration,\n",
    "            \"Num steps:\", num_steps_in_episode,\n",
    "            \"Reward:\", episode_reward,\n",
    "            #\"Training time per step:\", \"%.3f\" % time_per_step,\n",
    "            \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg,\n",
    "            #\"Epsilon:\", \"%.3f\" % epsilon\n",
    "        )\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating experience replay buffer...\n",
      "Episode: 0 Num steps: 354 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 1 Num steps: 275 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 2 Num steps: 315 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "coping to the target_network\n",
      "Episode: 3 Num steps: 605 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 4 Num steps: 384 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 5 Num steps: 452 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 6 Num steps: 422 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 7 Num steps: 311 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 8 Num steps: 382 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 9 Num steps: 330 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 10 Num steps: 312 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 11 Num steps: 329 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 12 Num steps: 469 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 13 Num steps: 399 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 14 Num steps: 408 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 15 Num steps: 394 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 16 Num steps: 453 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 17 Num steps: 339 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 18 Num steps: 262 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 19 Num steps: 434 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 20 Num steps: 447 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 21 Num steps: 472 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 22 Num steps: 591 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 23 Num steps: 377 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 24 Num steps: 439 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 25 Num steps: 329 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 26 Num steps: 435 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 27 Num steps: 479 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 28 Num steps: 471 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 29 Num steps: 446 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 30 Num steps: 389 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 31 Num steps: 497 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 32 Num steps: 371 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 33 Num steps: 269 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 34 Num steps: 350 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 35 Num steps: 530 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 36 Num steps: 298 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 37 Num steps: 255 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 38 Num steps: 268 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "coping to the target_network\n",
      "Episode: 39 Num steps: 814 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 40 Num steps: 393 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 41 Num steps: 743 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 42 Num steps: 473 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 43 Num steps: 414 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 44 Num steps: 334 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 45 Num steps: 426 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 46 Num steps: 344 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 47 Num steps: 333 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 48 Num steps: 305 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 49 Num steps: 220 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 50 Num steps: 259 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 51 Num steps: 264 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 52 Num steps: 277 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 53 Num steps: 360 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 54 Num steps: 527 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 55 Num steps: 444 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 56 Num steps: 322 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 57 Num steps: 264 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 58 Num steps: 500 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 59 Num steps: 565 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 60 Num steps: 253 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 61 Num steps: 470 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 62 Num steps: 234 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 63 Num steps: 438 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 64 Num steps: 450 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 65 Num steps: 312 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 66 Num steps: 304 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 67 Num steps: 413 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 68 Num steps: 648 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 69 Num steps: 230 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 70 Num steps: 450 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 71 Num steps: 384 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 72 Num steps: 374 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 73 Num steps: 349 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 74 Num steps: 591 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 75 Num steps: 285 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 76 Num steps: 576 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 77 Num steps: 446 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 78 Num steps: 388 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 79 Num steps: 411 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 80 Num steps: 304 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 81 Num steps: 390 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 82 Num steps: 357 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 83 Num steps: 348 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 84 Num steps: 300 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 85 Num steps: 558 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 86 Num steps: 278 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 87 Num steps: 539 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 88 Num steps: 241 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 89 Num steps: 330 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 90 Num steps: 274 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 91 Num steps: 365 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 92 Num steps: 369 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 93 Num steps: 318 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 94 Num steps: 286 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 95 Num steps: 313 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 96 Num steps: 301 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 97 Num steps: 310 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 98 Num steps: 528 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 99 Num steps: 274 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 100 Num steps: 417 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 101 Num steps: 458 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 102 Num steps: 355 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 103 Num steps: 333 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 104 Num steps: 332 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 105 Num steps: 488 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 106 Num steps: 460 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 107 Num steps: 403 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 108 Num steps: 440 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 109 Num steps: 453 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 110 Num steps: 395 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 111 Num steps: 466 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 112 Num steps: 384 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 113 Num steps: 437 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 114 Num steps: 485 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 115 Num steps: 479 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 116 Num steps: 262 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 117 Num steps: 299 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 118 Num steps: 453 Reward: 0.1 Avg Reward (Last 100): 0.100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 119 Num steps: 430 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 120 Num steps: 397 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 121 Num steps: 499 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 122 Num steps: 568 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 123 Num steps: 550 Reward: 0.1 Avg Reward (Last 100): 0.100\n",
      "Episode: 124 Num steps: 197 Reward: 0.1 Avg Reward (Last 100): 0.100\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b18ea5a673fa>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0meps_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mcopy_period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         )\n\u001b[1;32m     68\u001b[0m         \u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1334ff1c3095>\u001b[0m in \u001b[0;36mplay_one\u001b[0;34m(env, model, tmodel, eps, eps_step, gamma, copy_period)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# update the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_experience\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0miters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1df3eed18e27>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, target_network)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             }\n\u001b[1;32m     99\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1091\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
