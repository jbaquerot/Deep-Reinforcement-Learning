{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from q_learning_bins import plot_running_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so you can test different architectures\n",
    "class HiddenLayer:\n",
    "    def __init__(self, M1, M2, f= T.tanh, use_bias= True):\n",
    "        self.W = theano.shared(np.random.randn(M1, M2) / np.sqrt(M1+M2))\n",
    "        self.params = [self.W]\n",
    "        self.use_bias = use_bias\n",
    "        if use_bias:\n",
    "            self.b = theano.shared(np.zeros(M2))\n",
    "            self.params += [self.b]\n",
    "        self.f = f\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.use_bias:\n",
    "            a = X.dot(self.W) + self.b\n",
    "        else:\n",
    "            a = X.dot(self.W)\n",
    "        return self.f(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximates pi(a | s)\n",
    "class PolicyModel:\n",
    "    def __init__(self, D, K, hidden_layer_sizes):\n",
    "        # starting learning rate and other hipermarameters\n",
    "        lr = 10e-4\n",
    "        mu = 0.7\n",
    "        decay = 0.999\n",
    "        \n",
    "        # create the graph\n",
    "        # K = number of actions\n",
    "        self.layers = []\n",
    "        M1 = D\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.layers.append(layer)\n",
    "            M1 = M2\n",
    "            \n",
    "        # final layer\n",
    "        layer = HiddenLayer(M1, K, lambda x: x, use_bias= False)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        # get all params for gradient later\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.params\n",
    "            \n",
    "        velocities = [theano.shared(p.get_value()*0) for p in params]\n",
    "        cache = [theano.shared(np.ones_like(p.get_value()*0.1)) for p in params]\n",
    "        \n",
    "        # inputs and targets\n",
    "        X = T.matrix('X')\n",
    "        actions = T.ivector('actions')\n",
    "        advantages = T.vector('advantages')\n",
    "          \n",
    "        # calculate output and cost\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        action_scores = Z\n",
    "        p_a_given_s = T.nnet.softmax(action_scores)\n",
    "        \n",
    "        selected_probs = T.log(p_a_given_s[T.arange(actions.shape[0]), actions])\n",
    "        cost = - T.sum(advantages * selected_probs)\n",
    "        \n",
    "        # specify update rule\n",
    "        grads = T.grad(cost, params)\n",
    "        g_update = [(p, p + v) for p, v, g in zip(params, velocities, grads)]\n",
    "        c_update = [(c, decay * c + (1 - decay) * g*g) for c, g in zip(cache, grads)]\n",
    "        v_update = [(v, mu*v - lr*g / T.sqrt(c)) for v, c, g in zip(velocities, cache, grads)]\n",
    "        # momentum option\n",
    "        # v_update = [(v, mu*v - lr*g) for v, g in zip(velocities, grads)]\n",
    "        # c_update = []\n",
    "        \n",
    "        updates = c_update + g_update + v_update\n",
    "        \n",
    "        # compile functions\n",
    "        self.train_op = theano.function(\n",
    "            inputs = [X, actions, advantages],\n",
    "            updates = updates,\n",
    "            allow_input_downcast = True\n",
    "        )\n",
    "        \n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [X],\n",
    "            outputs = p_a_given_s,\n",
    "            allow_input_downcast = True\n",
    "        )\n",
    "        \n",
    "    def partial_fit(self, X, actions, advantages):\n",
    "        X = np.atleast_2d(X)\n",
    "        actions = np.atleast_1d(actions)\n",
    "        advantages = np.atleast_1d(advantages)\n",
    "        self.train_op(X, actions, advantages)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        return self.predict_op(X)\n",
    "    \n",
    "    def sample_action(self, X):\n",
    "        p = self.predict(X)[0]\n",
    "        nonans = np.all(~np.isnan(p))\n",
    "        assert(nonans)\n",
    "        return np.random.choice(len(p), p = p)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximates V(s)\n",
    "class ValueModel:\n",
    "    def __init__(self, D, hidden_layer_sizes):\n",
    "        # constant learning rate is fine\n",
    "        lr = 10e-5\n",
    "        \n",
    "        # create the graph\n",
    "        self.layers = []\n",
    "        M1 = D\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.layers.append(layer)\n",
    "            M1 = M2\n",
    "        \n",
    "        # final layer\n",
    "        layer = HiddenLayer(M1, 1, lambda x: x)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        # get all params for gradient later\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.params\n",
    "            \n",
    "        # inputs and targets\n",
    "        X = T.matrix('X')\n",
    "        Y = T.vector('Y')\n",
    "        \n",
    "        # calculate output and cost\n",
    "        Z = X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        Y_hat = T.flatten(Z)\n",
    "        cost = T.sum((Y - Y_hat) ** 2)\n",
    "        \n",
    "        # sepecify update rule\n",
    "        grads = T.grad(cost, params)\n",
    "        updates = [(p, p - lr * g) for p, g in zip(params, grads)]\n",
    "        \n",
    "        # compile functions\n",
    "        self.train_op = theano.function(\n",
    "            inputs = [X, Y],\n",
    "            updates = updates,\n",
    "            allow_input_downcast = True\n",
    "        )\n",
    "        \n",
    "        self.predict_op = theano.function(\n",
    "            inputs = [X],\n",
    "            outputs = Y_hat,\n",
    "            allow_input_downcast = True\n",
    "        )\n",
    "                \n",
    "        \n",
    "    def partial_fit(self, X, Y):\n",
    "        X = np.atleast_2d(X)\n",
    "        Y = np.atleast_1d(Y)\n",
    "        self.train_op(X, Y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        return self.predict_op(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_mc(env, pmodel, vmodel, gamma):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    \n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    while not done and iters < 2000:\n",
    "        # if we reach 2000, just quit, don't want this going forever\n",
    "        # the 200 limit seems a bit early\n",
    "        action = pmodel.sample_action(observation)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        #if done:\n",
    "        #    reward = -200\n",
    "            \n",
    "        states.append(prev_observation)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if reward == 1: # if we changed the reward to -200\n",
    "            totalreward += reward\n",
    "            \n",
    "        iters +=1\n",
    "        \n",
    "    returns = []\n",
    "    advantages = []\n",
    "    G = 0\n",
    "    for s, r in zip(reversed(states), reversed(rewards)):\n",
    "        returns.append(G)\n",
    "        advantages.append(G - vmodel.predict(s)[0])\n",
    "        G = r + gamma * G\n",
    "        \n",
    "    returns.reverse()\n",
    "    advantages.reverse()\n",
    "    \n",
    "    # update the models\n",
    "    pmodel.partial_fit(states, actions, advantages)\n",
    "    vmodel.partial_fit(states, returns)\n",
    "    \n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    D = env.observation_space.shape[0]\n",
    "    K = env.action_space.n\n",
    "    pmodel = PolicyModel(D, K, [])\n",
    "    vmodel = ValueModel(D, [10])\n",
    "    gamma = 0.99\n",
    "    \n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "        \n",
    "    N = 1000\n",
    "    totalrewards = np.empty(N)\n",
    "    for n in range(N):\n",
    "        totalreward = play_one_mc(env, pmodel, vmodel, gamma)\n",
    "        totalrewards[n] = totalreward\n",
    "        if n % 100 == 0:\n",
    "            print(\"episode:\", n, \"total reward:\", totalreward, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "            \n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "    print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "    plt.plot(totalrewards)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "\n",
    "    plot_running_avg(totalrewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
