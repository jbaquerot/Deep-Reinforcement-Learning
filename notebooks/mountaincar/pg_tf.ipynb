{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from datetime import datetime\n",
    "from q_learning import plot_running_avg, plot_cost_to_go, FeatureTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so you can test different architectures\n",
    "class HiddenLayer:\n",
    "    def __init__(self, M1, M2, f=tf.nn.tanh, use_bias= True, zeros= False):\n",
    "        if zeros:\n",
    "            W = np.zeros((M1, M2), dtype = np.float32)    \n",
    "        else: \n",
    "            W = tf.random_normal(shape = (M1, M2)) * np.sqrt(2./ M1, dtype = np.float32)\n",
    "        self.W = tf.Variable(W)\n",
    "\n",
    "        self.use_bias = use_bias\n",
    "        if use_bias:\n",
    "            self.b = tf.Variable(np.zeros(M2).astype(np.float32))\n",
    "        \n",
    "        self.f = f\n",
    "    \n",
    "    def forward(self, X):\n",
    "        if self.use_bias:\n",
    "            a = tf.matmul(X, self.W) + self.b\n",
    "        else:\n",
    "            a = tf.matmul(X, self.W)\n",
    "        return self.f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximates pi(a | s)\n",
    "class PolicyModel:\n",
    "    def __init__(self, D, ft, hidden_layer_sizes= []):\n",
    "        self.ft = ft\n",
    "        \n",
    "        ### hidden layers ####\n",
    "        M1 = D\n",
    "        self.hidden_layers = []\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.hidden_layers.append(layer)\n",
    "            M1 = M2\n",
    "        \n",
    "        # final layer mean\n",
    "        self.mean_layer = HiddenLayer(M1, 1, lambda x: x, use_bias = False, zeros = True)\n",
    "         \n",
    "        # final layer variance\n",
    "        self.stdv_layer = HiddenLayer(M1, 1, tf.nn.softplus, use_bias = False, zeros = False)\n",
    "\n",
    "        # inputs and targets\n",
    "        self.X = tf.placeholder(tf.float32, shape= (None, D), name= 'X')\n",
    "        self.actions = tf.placeholder(tf.float32, shape= (None,), name='actions')\n",
    "        self.advantages = tf.placeholder(tf.float32, shape= (None,), name= 'advantages')\n",
    "        \n",
    "        # get final hidden layer\n",
    "        Z = self.X\n",
    "        for layer in self.hidden_layers:\n",
    "            Z = layer.forward(Z)\n",
    "            \n",
    "         \n",
    "        # calculate output and cost  \n",
    "        mean = self.mean_layer.forward(Z)\n",
    "        stdv = self.stdv_layer.forward(Z) + 10e-5 # smoothing\n",
    "        \n",
    "        #make then 1-D\n",
    "        mean = tf.reshape(mean, [-1])\n",
    "        stdv = tf.reshape(stdv, [-1])\n",
    "        \n",
    "        norm = tf.contrib.distributions.Normal(mean, stdv)\n",
    "        self.predict_op = tf.clip_by_value(norm.sample(), -1, 1)\n",
    "        \n",
    "        log_probs= norm.log_prob(self.actions)\n",
    "        cost = -tf.reduce_sum(self.advantages * log_probs + 0.1*norm.entropy())\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-3).minimize(cost)\n",
    "        \n",
    "        \n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "        \n",
    "    def partial_fit(self, X, actions, advantages):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        \n",
    "        actions = np.atleast_1d(actions)\n",
    "        advantages = np.atleast_1d(advantages)\n",
    "        self.session.run(\n",
    "            self.train_op,\n",
    "            feed_dict= {\n",
    "                self.X: X,\n",
    "                self.actions: actions,\n",
    "                self.advantages: advantages,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        return self.session.run(self.predict_op, feed_dict={self.X: X})\n",
    "    \n",
    "    def sample_action(self, X):\n",
    "        p = self.predict(X)[0]\n",
    "        return p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximates V(s)\n",
    "class ValueModel:\n",
    "    def __init__(self, D, ft, hidden_layer_sizes= []):\n",
    "        self.ft = ft\n",
    "        self.costs = []\n",
    "        \n",
    "        #create the graph\n",
    "        self.layers = []\n",
    "        M1 = D\n",
    "        for M2 in hidden_layer_sizes:\n",
    "            layer = HiddenLayer(M1, M2)\n",
    "            self.layers.append(layer)\n",
    "            M1 = M2\n",
    "            \n",
    "        # final layer\n",
    "        layer = HiddenLayer(M1, 1, lambda x: x)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        # inputs and targets\n",
    "        self.X = tf.placeholder(tf.float32, shape= (None, D), name= 'X')\n",
    "        self.Y = tf.placeholder(tf.float32, shape= (None, ), name= 'Y')\n",
    "        \n",
    "        # calculata output and cost\n",
    "        Z = self.X\n",
    "        for layer in self.layers:\n",
    "            Z = layer.forward(Z)\n",
    "        Y_hat = tf.reshape(Z, [-1]) #the output\n",
    "        self.predict_op = Y_hat\n",
    "        \n",
    "        cost = tf.reduce_sum(tf.square(self.Y - Y_hat))\n",
    "        self.cost = cost\n",
    "        self.train_op = tf.train.AdamOptimizer(1e-1).minimize(cost)\n",
    "        \n",
    "    def set_session(self, session):\n",
    "        self.session = session\n",
    "        \n",
    "    def partial_fit(self, X, Y):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        Y = np.atleast_1d(Y)\n",
    "        self.session.run(self.train_op, feed_dict= {self.X: X, self.Y: Y})\n",
    "        cost = self.session.run(self.cost, feed_dict= {self.X: X, self.Y: Y})\n",
    "        self.costs.append(cost)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = np.atleast_2d(X)\n",
    "        X = self.ft.transform(X)\n",
    "        return self.session.run(self.predict_op, feed_dict= {self.X: X})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_td(env, pmodel, vmodel, gamma):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    totalreward = 0\n",
    "    iters = 0\n",
    "    \n",
    "    while not done and iters < 2000:\n",
    "        # if we reach 2000, just quit, don't want this going forever\n",
    "        # the 200 limit seems a bit early\n",
    "        action = pmodel.sample_action(observation)\n",
    "        prev_observation = observation\n",
    "        observation, reward, done, info = env.step([action])\n",
    "        \n",
    "        totalreward += reward   \n",
    "        \n",
    "        # update the models\n",
    "        V_next = vmodel.predict(observation)\n",
    "        G = reward + gamma * V_next\n",
    "        advantage = G - vmodel.predict(prev_observation)\n",
    "        pmodel.partial_fit(prev_observation, action, advantage)\n",
    "        vmodel.partial_fit(prev_observation, G)\n",
    "        \n",
    "        iters +=1\n",
    "    \n",
    "    return totalreward, iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('MountainCarContinuous-v0')\n",
    "    ft = FeatureTransformer(env, n_components= 100)\n",
    "    D = ft.dimensions\n",
    "    pmodel = PolicyModel(D, ft, [])\n",
    "    vmodel = ValueModel(D, ft, [])\n",
    "    init = tf.global_variables_initializer()\n",
    "    session = tf.InteractiveSession()\n",
    "    session.run(init)\n",
    "    pmodel.set_session(session)\n",
    "    vmodel.set_session(session)\n",
    "    gamma = 0.95\n",
    "    \n",
    "    if 'monitor' in sys.argv:\n",
    "        filename = os.path.basename(__file__).split('.')[0]\n",
    "        monitor_dir = './' + filename + '_' + str(datetime.now())\n",
    "        env = wrappers.Monitor(env, monitor_dir)\n",
    "        \n",
    "        \n",
    "    N = 50\n",
    "    totalrewards = np.empty(N)\n",
    "    costs = np.empty(N)\n",
    "    for n in range(N):\n",
    "        totalreward, num_steps = play_one_td(env, pmodel, vmodel, gamma)\n",
    "        totalrewards[n] = totalreward\n",
    "        if n % 1 == 0:\n",
    "            print(\"episode:\", n, \"total reward: %.1f\" % totalreward, \"num steps: %d\" % num_steps, \n",
    "                 \"avg reward (last 100): %.1f\" % totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "    print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "    \n",
    "    \n",
    "    plt.plot(totalrewards)\n",
    "    plt.title(\"Rewards\")\n",
    "    plt.show()\n",
    "    \n",
    "    plot_running_avg(totalrewards)\n",
    "    plot_cost_to_go(env, vmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 59.7 num steps: 586 avg reward (last 100): 59.7\n",
      "episode: 1 total reward: -80.4 num steps: 999 avg reward (last 100): -10.3\n",
      "episode: 2 total reward: 42.2 num steps: 746 avg reward (last 100): 7.2\n",
      "episode: 3 total reward: 21.9 num steps: 944 avg reward (last 100): 10.9\n",
      "episode: 4 total reward: 63.6 num steps: 430 avg reward (last 100): 21.4\n",
      "episode: 5 total reward: 57.5 num steps: 475 avg reward (last 100): 27.4\n",
      "episode: 6 total reward: 60.6 num steps: 443 avg reward (last 100): 32.2\n",
      "episode: 7 total reward: 52.0 num steps: 544 avg reward (last 100): 34.6\n",
      "episode: 8 total reward: 63.5 num steps: 409 avg reward (last 100): 37.9\n",
      "episode: 9 total reward: 80.2 num steps: 231 avg reward (last 100): 42.1\n",
      "episode: 10 total reward: 77.4 num steps: 262 avg reward (last 100): 45.3\n",
      "episode: 11 total reward: 37.8 num steps: 707 avg reward (last 100): 44.7\n",
      "episode: 12 total reward: 65.5 num steps: 394 avg reward (last 100): 46.3\n",
      "episode: 13 total reward: 62.9 num steps: 442 avg reward (last 100): 47.5\n",
      "episode: 14 total reward: 65.4 num steps: 410 avg reward (last 100): 48.7\n",
      "episode: 15 total reward: 72.3 num steps: 314 avg reward (last 100): 50.1\n",
      "episode: 16 total reward: 79.2 num steps: 245 avg reward (last 100): 51.9\n",
      "episode: 17 total reward: 66.2 num steps: 380 avg reward (last 100): 52.7\n",
      "episode: 18 total reward: 62.8 num steps: 406 avg reward (last 100): 53.2\n",
      "episode: 19 total reward: 80.0 num steps: 240 avg reward (last 100): 54.5\n",
      "episode: 20 total reward: 81.9 num steps: 220 avg reward (last 100): 55.8\n",
      "episode: 21 total reward: 31.7 num steps: 775 avg reward (last 100): 54.7\n",
      "episode: 22 total reward: 86.9 num steps: 148 avg reward (last 100): 56.1\n",
      "episode: 23 total reward: 77.1 num steps: 262 avg reward (last 100): 57.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
